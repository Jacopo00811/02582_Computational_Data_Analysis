{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()  # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(\"sand.mat\")\n",
    "X = mat[\"X\"]\n",
    "y = mat[\"Y\"].ravel()\n",
    "\n",
    "[n, p] = X.shape\n",
    "\n",
    "\n",
    "def centerData(data):\n",
    "\n",
    "    mu = np.mean(data, axis=0)\n",
    "    data = data - mu\n",
    "\n",
    "    return data, mu\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"\n",
    "    Function for normalizing the columns (variables) of a data matrix to unit length.\n",
    "    Returns the normalized data and the euclidian lenghts of the variables\n",
    "\n",
    "    Input  (X) --------> The data matrix to be normalized\n",
    "    Output (X_pre)-----> The normalized data matrix\n",
    "    Output (d) --------> Array with the euclidian lenghts of the variables\n",
    "    \"\"\"\n",
    "    d = np.linalg.norm(X, axis=0, ord=2)  # d is the the L2 norms of the variables\n",
    "    d[d == 0] = 1  # Avoid dividing by zero if column L2 norm is 0\n",
    "    X_pre = X / d  # Normalize the data with the euclidian lengths\n",
    "    return X_pre, d  # Return normalized data and the euclidian lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Perform univariate feature selection for the sand data using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Bonferroni correction to control the family-wise error rate(FWER). Use FWER = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after correcting with Bonferroni correction: 72.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the pvalue for each feature one at the time because OLS breaks down with this many features\n",
    "PValues = np.zeros(p)\n",
    "Xsub = np.zeros(p)\n",
    "for j in range(p):\n",
    "    Xsub = X[:, j]\n",
    "    # Use the stats models linear regression, since p value already is included\n",
    "    # Otherwise check https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "    # Which explains how to expand the class in sklearn to calculate it\n",
    "    slope, intercept, r_value, PValues[j], std_err = linregress(Xsub, y)\n",
    "\n",
    "# Sort p-values in acending order\n",
    "idx1 = np.argsort(PValues)\n",
    "p = PValues[idx1]\n",
    "\n",
    "# include all features with p values lower  than p / features\n",
    "remaining_features_bonf = len(\n",
    "    np.where(p < (0.05 / 2016))[0]\n",
    ")  # Amount af features included\n",
    "print(\n",
    "    f\"Remaining features after correcting with Bonferroni correction: {remaining_features_bonf}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Benjamini-Hochbergâ€™s algorithm for FDR. Use an acceptable fraction of mistakes,\n",
    "q = 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after applying DFR: 721.\n"
     ]
    }
   ],
   "source": [
    "# Use multipletests to get the FDR corrected p values\n",
    "FDR = multipletests(PValues, alpha=0.05, method=\"fdr_bh\")[\n",
    "    1\n",
    "]  # Computing Benjamini Hochberg's FDR\n",
    "q = 0.15\n",
    "# Sort p-values in acending order\n",
    "\n",
    "idx2 = np.argsort(FDR)\n",
    "fdr = FDR[idx2]\n",
    "\n",
    "# include all features with p values lower than q\n",
    "remaining_features_fdr = len(np.where(fdr < q)[0])  # How many values are below 0.15?\n",
    "print(f\"Remaining features after applying DFR: {remaining_features_fdr}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the solutions in terms of number of selected features and selected features.\n",
    "\n",
    "\n",
    "FDR keeps more featurs in the model. This has the con of having false discoveries but the pro to make sure that all significant features are kept in the model. Bonferroni might remove some significant features because of the more stringent cutoff but has less false discoveries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
