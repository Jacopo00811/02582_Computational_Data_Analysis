{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To embed plots in the notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # numpy library\n",
    "import scipy . linalg as lng # linear algebra from scipy library\n",
    "from scipy . spatial import distance # load distance function\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetPath = \"./DiabetesDataNormalized.txt\"\n",
    "T = np.loadtxt(diabetPath, delimiter=\" \", skiprows=1)\n",
    "y = T[:, 10]\n",
    "X = T[:, :10]\n",
    "\n",
    "# Get number of observations (n) and number of independent variables (p)\n",
    "[n, p] = np.shape(X)\n",
    "\n",
    "M = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Solve the Ordinary Least Squares (OLS) computationally (for the diabetes data set):\n",
    "\n",
    "> (a) What is the difference between using a brute force implementation (analytical) for an OLS solver and a numerically ’smarter’ implementation? Compute the ordinary least squares solution to the diabetes data set for both options and look at the relative difference. Use for example lng.lstsq to invert the matrix or to solve the linear system of equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_numerical(X, y):\n",
    "    # Call lstsq from lng to get betas\n",
    "    beta, _, _, _ = lng.lstsq(X, y)\n",
    "    return beta\n",
    "\n",
    "\n",
    "def ols_analytical(X, y):\n",
    "    # Implement the analytical closed form way of calculating the betas\n",
    "    X_transpose = np.transpose(X)\n",
    "    beta = np.linalg.inv(X_transpose @ X) @ X_transpose @ y\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    }
   ],
   "source": [
    "# numerical solution\n",
    "beta_num = ols_numerical(M, y)\n",
    "print(f\"The list of betas: \\n{beta_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution\n",
    "beta_ana = ols_analytical(M, y)\n",
    "print(f\"The list of betas: \\n{beta_ana}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of the difference between betas: \n",
      "2.2328026633969272e-14\n"
     ]
    }
   ],
   "source": [
    "# difference in solutions\n",
    "norm = np.linalg.norm(beta_ana - beta_num)\n",
    "print(f\"The norm of the difference between betas: \\n{norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the difference significant?\n",
    "\n",
    "What can we conclude relating to numerical vs analytical solutions?\n",
    "\n",
    "The difference between the numerical and analytical solutions is negligible. This indicates that both methods are reliable for solving the Ordinary Least Squares (OLS) problem. However, the numerical method using `lstsq` is generally preferred for larger datasets because it is more stable and less prone to numerical errors compared to the analytical method, which involves matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) How could you include an intercept term in Python? This means using the model: $y = β_0 +xβ_1 +...+x_pβ_p +ε $ rather than: $ y=x_1β_1 +...+x_pβ_p +ε. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.39787702e-16, -6.18292545e-03, -1.48130075e-01,  3.21100050e-01,\n",
       "        2.00366920e-01, -4.89313521e-01,  2.94473646e-01,  6.24127211e-02,\n",
       "        1.09368973e-01,  4.64049083e-01,  4.17718663e-02])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include offset / intercept adding a column of ones to the matrix\n",
    "M = np.column_stack((np.ones(n), X))\n",
    "# Recalculate beta_ana after adding the intercept term\n",
    "beta_ana = ols_analytical(M, y)\n",
    "beta_ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value of the intercept coefficient?\n",
    "\n",
    "Can you explain why?\n",
    "\n",
    "The value of the intercept is -6.39787702e-16. The intercept is very close to zero due to the nature of the data preprocessing (centering and scaling) and the precision of the numerical computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Calculate the mean squared error $MSE = 1/n \\sum^n_{i=1} (y_i−x_iβ)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated y values and use these to calculate the MSE.\n",
    "def compute_mse(X, beta):\n",
    "    y_est = X @ beta\n",
    "    mse = np.mean((y - y_est) ** 2)\n",
    "    return mse, y, y_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the analytical solution: 0.48116051086159695\n"
     ]
    }
   ],
   "source": [
    "mse_ana, res_ana, yhat_ana = compute_mse(M, beta_ana)\n",
    "print(f\"mse from the analytical solution: {mse_ana}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the MSE if we change some of the betas?\n",
    "\n",
    "Is that what you expected?\n",
    "\n",
    "When we change some of the betas, the MSE (Mean Squared Error) increases. This is expected because altering the betas from their optimal values (as calculated by the OLS method) leads to a less accurate model, resulting in higher prediction errors and thus a higher MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the changed betas: 0.7200465405399794\n"
     ]
    }
   ],
   "source": [
    "beta_new = beta_ana\n",
    "beta_new[5] = 0\n",
    "\n",
    "mse_new, res_new, yhat_new = compute_mse(M, beta_new)\n",
    "\n",
    "print(f\"mse from the changed betas: {mse_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) Calculate the residual sum of squares $RSS = ||{\\bf y} - X \\beta||_2^2$ and the total sum of squares $TSS = ||{\\bf y} - \\bar{y}||_2^2$, where $\\bar{y}$ is the estimated mean of ${\\bf y}$. Report on the $R^2$ measure, that is, the proportion of variance in the sample set explained by the model: $R^2 = 1 - \\frac{RSS}{TSS}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5177484222203499)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss = np.sum((y - M @ beta_ana) ** 2)\n",
    "tss = np.sum((y - np.mean(y)) ** 2)\n",
    "r2 = 1 - rss / tss\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much variance in <strong>y</strong> can we explain using this simple model?\n",
    "\n",
    "The R-squared value for our model is 0.51775. This indicates that approximately 51.77% of the variance in y can be explained by this simple model."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
