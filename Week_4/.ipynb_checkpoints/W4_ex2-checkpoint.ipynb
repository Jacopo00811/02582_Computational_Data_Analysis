{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import warnings # to silence convergence warnings\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data to pandas dataframes and convert to numpy arrays\n",
    "GXtrain = pd.read_csv(os.path.join('..','Data', 'GolubGXtrain.csv'), header=None)\n",
    "GXtest = pd.read_csv(os.path.join('..','Data', 'GolubGXtest.csv'), header=None)\n",
    "\n",
    "Xtrain = np.array(GXtrain.loc[:, GXtrain.columns != 0])\n",
    "Ytrain = np.array(GXtrain.loc[:, GXtrain.columns == 0]).ravel()\n",
    "\n",
    "Xtest = np.array(GXtest.loc[:, GXtest.columns != 0])\n",
    "Ytest = np.array(GXtest.loc[:, GXtest.columns == 0]).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 We have a data material (Golub et al 1999) with gene expression levels from 72 patients with two forms of leukemia, acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). Gene expression levels (how actively the cells are using the information in di\u000b",
    "erent genes) are measured for 7127 genes. We would like to build a biomarker for classification of the two cancer forms. Ideally, we would like to use onlya few variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) How can you use logistic regression here?\n",
    "\n",
    "> b) Build a classifier for training data in GolubGXtrain.csv. What regularization method do you prefer if you want to have few genes in the biomarker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devianceFunc(ytrue, yscores):\n",
    "    return 2*sk.metrics.log_loss(ytrue, yscores, normalize=False)\n",
    "\n",
    "lambdas = np.logspace(-4,2,50)\n",
    "K = 10\n",
    "CV = #Select CrossValidation Strategy\n",
    "X_standardizer = #Select normalization strategy\n",
    "\n",
    "deviance = np.zeros((K, len(lambdas)))\n",
    "with warnings.catch_warnings(): # done to disable all the convergence warnings from elastic net\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    #Setup CrossValidation loop\n",
    "\n",
    "\n",
    "        #Setup loop over possible lambdas\n",
    "        \n",
    "        \n",
    "            #For each lambda run logistic regression\n",
    "\n",
    "            \n",
    "            #Predict the data in the test fold\n",
    "            y_est = \n",
    "            \n",
    "            #compute the deviance\n",
    "            deviance[i,k] = \n",
    "            \n",
    "#Compute the average and std of the deviance over CV folds\n",
    "testError = \n",
    "testStd =\n",
    "\n",
    "#Select the optimal lambda using the 1-std-rule\n",
    "Lambda_CV_1StdRule = \n",
    "print(\"CV lambda 1 std rule %0.2f\" % Lambda_CV_1StdRule)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(lambdas, testError, testStd, marker='.', color='orange', markersize=10)\n",
    "plt.semilogx(lambdas, testError)\n",
    "\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "plt.title(\"Cross-validated deviance of Lasso fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) How many variables do you end up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we know our optimal lambda we can create our model with our training set\n",
    "\n",
    "nrCoefs = \n",
    "\n",
    "print(\"The number of non-zero coefficients in our optimal model is: %d\" % nrCoefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Use the obtained model to calculate accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the class using the model trained above and calculate the accuracy\n",
    "\n",
    "accuracy = \n",
    "\n",
    "print(\"The accuracy for our optimal model is: %0.2f\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
